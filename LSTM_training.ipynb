{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12976\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from preprocess import load_data,load_dict\n",
    "import pandas as pd\n",
    "\n",
    "train_path = './essay_dataset/training_set_rel3.tsv'\n",
    "training_data = load_data(train_path)\n",
    "training_data.dropna(subset=['essay_set','domain1_score', 'essay'],how='any',inplace = True)\n",
    "training_data = training_data[['essay_set','domain1_score', 'domain2_score','essay']]\n",
    "training_data.domain1_score = training_data.domain1_score.astype('float64')\n",
    "\n",
    "glove_dict = load_dict('glove.840B.300d.pkl')\n",
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "word2ind = {}\n",
    "for text in training_data.essay:\n",
    "    lowered = ''.join([(' '+c+' ') if c in punctuation else c for c in text.lower() ])\n",
    "    sequence = lowered.split()\n",
    "    for word in sequence:\n",
    "        if word not in word2ind:\n",
    "            word2ind[word] = len(word2ind)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39759"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_path = './essay_dataset/valid_set.tsv'\n",
    "valid_label_path = './essay_dataset/valid_sample_submission_2_column.csv'\n",
    "valid_data = load_data(valid_path)\n",
    "valid_label = pd.read_csv(valid_label_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = dict(zip(valid_label.prediction_id, valid_label.predicted_score))\n",
    "for (i,row) in valid_data.iterrows():\n",
    "    valid_data.at[i,'domain1_predictionid'] = label_dict[row['domain1_predictionid']]\n",
    "valid_data = valid_data[['essay_set','essay','domain1_predictionid']]\n",
    "valid_data = valid_data.rename(index=str, columns={'domain1_predictionid':'domain1_score'})\n",
    "valid_data.domain1_score = valid_data.domain1_score.astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_scores(data, score_domain):\n",
    "    \n",
    "    for (i,row) in data.iterrows():\n",
    "        col = score_domain\n",
    "        if row['essay_set'] == 1:\n",
    "            data.at[i, col] = row[score_domain] - 2\n",
    "        elif row['essay_set'] == 2:\n",
    "            data.at[i, col] =(row[score_domain] - 1)*2\n",
    "        elif row['essay_set'] == 3 or row['essay_set'] == 4:\n",
    "            data.at[i, col] =row[score_domain]/3.0*10\n",
    "            \n",
    "        elif row['essay_set'] == 5 or row['essay_set'] == 6:\n",
    "            data.at[i, col]= row[score_domain]/4.0*10\n",
    "            \n",
    "        elif row['essay_set'] == 7:\n",
    "            data.at[i, col] =row[score_domain]/3.0\n",
    "            \n",
    "        elif row['essay_set'] == 8:\n",
    "            data.at[i, col] =row[score_domain]/6.0\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = process_scores(training_data, 'domain1_score')\n",
    "valid_data = process_scores(valid_data, 'domain1_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs import cfg\n",
    "\n",
    "cfg['dropout'] = 0.005 # dropout rate between two layers of LSTM; useful only when layers > 1; between 0 and 1\n",
    "cfg['bidirectional'] = True # True or False; True means using a bidirectional LSTM\n",
    "cfg['batch_size'] = 100 # batch size of input\n",
    "cfg['learning_rate'] = 1e-4 # learning rate to be used\n",
    "cfg['L2_penalty'] = 1e-4 # weighting constant for L2 regularization term; this is a parameter when you define optimizer\n",
    "cfg['epochs'] = 15 # number of epochs for which the model is trained\n",
    "cfg['embed'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ready\n"
     ]
    }
   ],
   "source": [
    "from dataloader import *\n",
    "from models import *\n",
    "import time\n",
    "\n",
    "model = LSTM_Score(cfg, True)\n",
    "\n",
    "model = model.to(torch.device(cfg['device']))\n",
    "criterion = nn.MSELoss()\n",
    "#optimizer = torch.optim.RMSprop(model.parameters(), lr=cfg['learning_rate'], weight_decay=cfg['L2_penalty'])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=cfg['learning_rate'], weight_decay=cfg['L2_penalty'])\n",
    "#train_indices, valid_indices = train_valid_split(len(training_data), cfg['train_split'])\n",
    "train_indices = list(range(len(training_data)))\n",
    "valid_indices = list(range(len(valid_data)))\n",
    "#np.save('val2.npy', valid_indices)\n",
    "print('ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count = 20, loss = 34.54701\n",
      "count = 40, loss = 32.89048\n",
      "count = 60, loss = 31.93664\n",
      "count = 80, loss = 31.66302\n",
      "count = 100, loss = 31.77691\n",
      "count = 120, loss = 31.14269\n",
      "validation loss: 30.69636903490339\n",
      "epoch finished: 1\n",
      "count = 20, loss = 30.64656\n",
      "count = 40, loss = 30.23009\n",
      "count = 60, loss = 28.48638\n",
      "count = 80, loss = 28.53532\n",
      "count = 100, loss = 27.80485\n",
      "count = 120, loss = 26.82688\n",
      "validation loss: 27.483848390125093\n",
      "epoch finished: 2\n",
      "count = 20, loss = 26.54094\n",
      "count = 40, loss = 26.59741\n",
      "count = 60, loss = 25.24757\n",
      "count = 80, loss = 26.23231\n",
      "count = 100, loss = 24.26094\n",
      "count = 120, loss = 25.03706\n",
      "validation loss: 24.546043305169967\n",
      "epoch finished: 3\n",
      "count = 20, loss = 23.83122\n",
      "count = 40, loss = 23.77767\n",
      "count = 60, loss = 23.34849\n",
      "count = 80, loss = 22.39374\n",
      "count = 100, loss = 22.99848\n",
      "count = 120, loss = 22.09768\n"
     ]
    }
   ],
   "source": [
    " for epoch in range(cfg['epochs']):\n",
    "    tloader = DataLoader(training_data, train_indices, cfg, glove_dict)\n",
    "    vloader = DataLoader(valid_data, valid_indices, cfg, glove_dict)\n",
    "    \n",
    "    count = 0\n",
    "    avg_loss = 0\n",
    "    while tloader.has_next():\n",
    "        train, label = tloader.get_next()\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        y = model(train)\n",
    "        \n",
    "        \n",
    "        loss = criterion(y, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        count += 1\n",
    "        avg_loss += loss.item()\n",
    "        if count % 20 == 0:\n",
    "            print(\"count = %d, loss = %.5f\" %(count, avg_loss / 20))\n",
    "            save_model(model, 'models_score/e' + str(epoch + 1) + 'b' + str(count) + '.pt')\n",
    "            avg_loss = 0\n",
    "        del train, label, y, loss\n",
    "    \n",
    "    count = 0\n",
    "    avg_loss = 0\n",
    "    with torch.no_grad():\n",
    "        while vloader.has_next():\n",
    "            train, label = vloader.get_next()\n",
    "            y = model(train)\n",
    "            loss = criterion(y, label)\n",
    "            count += 1\n",
    "            avg_loss += loss.item()\n",
    "            del train, label, y, loss\n",
    "    print('validation loss:', avg_loss / count)\n",
    "    print('epoch finished:', epoch + 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4052],\n",
      "        [ 4.6292],\n",
      "        [ 6.1769],\n",
      "        [-1.2990],\n",
      "        [ 0.8844]], device='cuda:0')\n",
      "validation loss: 12.443446159362793\n"
     ]
    }
   ],
   "source": [
    "cfg['batch_size'] = 5\n",
    "vloader = DataLoader(valid_data, valid_indices, cfg, glove_dict)\n",
    "count = 0\n",
    "avg_loss = 0\n",
    "with torch.no_grad():\n",
    "    while vloader.has_next():\n",
    "        train, label = vloader.get_next()\n",
    "        y = model(train)\n",
    "        #y = y.permute(0, 2, 1)\n",
    "        print(label - y)\n",
    "        loss = criterion(y, label)\n",
    "        count += 1\n",
    "        avg_loss += loss.item()\n",
    "        del train, label, y, loss\n",
    "        break\n",
    "print('validation loss:', avg_loss / count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
